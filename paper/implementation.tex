\TODO{link to repo}

\section{Berge graphs recognition: na\"ive approach}
\TODO{rewrite to describe naive}

\section{Berge graphs recognition: polynomial algorithm}
\todo{rewrite/rearrange text below}

The Berge recognition algorithm's running time is $O(n^9)$, which brings into question its applicability to any real use case. Although time complexity is indeed a limiting factor, a number of lower level optimizations done on implementation's level (\cref{sec:Optimizations}) make a very big difference and make it usable -- or at least much more usable than na\"ive algorithm could be. Also, we explore a new frontier of implementing its most time consuming part on massively parallel GPU architecture, with good results (\cref{sec:CUDA}).

\TODO{Anything interesting about algo/data structure?}

\subsection{Optimizations}
\label{sec:Optimizations}

When implementing a complicated algorithm that has a time complexity of $O(n^9)$ optimizations can be both crucial and difficult to implement. There is not a single code path that takes up all the running time -- or at lest there isn't one from theoretical point of view. Therefore a tool for inspecting running time bottlenecks is needed. We used Valgrind's tool called \emph{callgrind} \cite{callgrind}.

Callgrind is a profiling tool that records the call history of a program and presents it as a call-graph. When profiling a program, event counts (data reads, cache misses etc.) are attributed directly to the function that they occurred in. In addition to that, the costs are propagated to functions upwards the call stack. For example, say internal function $worker$ consumes most of programs running time. It is called from $run1$ and $run2$, with $run2$ calls taking twice as much time. $run1$ and $run2$ are in turn called from main. Total attribution of $worker$ would be nearly 100\%, of $run1$ about 33\% and of $run2$ about 66\%. The contribution of main would also be near 100\%. We used a tool \emph{gprof2dot} \cite{gprof2dot} to generate visual call graphs from callgrind's output.

\TODO{example of a call graph?}

\TODO{callgrind}
\TODO{first result to best result gains (like: we are 20x faster than what first comes to mind)}
\TODO{regenerate speedup results to make sure what the gains were}

\subsubsection{Generating paths}

\TODO{we also optimize na\"ive, in fact it makes a bigger difference there.}
A major part of Berge recognition algorithm is spent on enumerating all possible paths of given length -- this is done either to find a hole by itself, find a simple structure or check a possible near cleaner for amenable odd hole. Therefore a quick method for generating paths, a problem that could seem trivial at first, is a crucial part to optimize and we give it much attention.

First of all, there are many methods for generating all possible paths to choose from. We could simply enumerate all sequences of vertices without repetition and for each one of them check if it is a path (that is if all pairs of vertices next to each other are connected and that there are no chords), but this would be too slow and render our algorithm unusable.

We notice that nowhere in the algorithm we need all the paths at once. It would suffice for us to have a path, check if it has the properties we further require of it and then proceed to generate next path (in some ordering of paths). Therefore, we need a method that receives a path on the input (or a special flag indicating it should generate first path) and returns next path in some order (or a first path, or a code signaling all paths have been generated). As this method will be used many, many times we require of it to work in place with constant additional space.

With those requirements defined a simplest algorithm would be to implement a sort of a "counter" with base of $|V(G)|$. In short: we take a path on a input, increment its last vertex until it is a neighbor of one before last vertex. Then we check if generated sequence is a path (all vertices unique and no chords). If it is we return it and if we run out of vertices before a path is found, we increment one before last vertex until it is a neighbor of one vertex before it, set last vertex to first and continue the process. If still no path is found, we increment vertices closer to beginning of the path, until all a path is found or we check all candidates.

This method is significantly faster than simply enumerating all combinations, but still leaves a problem of generating paths a main bottleneck of our algorithm (in our preliminary test it took about 70\% of running time for graphs with $|V(G)| = 10$ and growing with bigger graphs. 

But we can do much better with some care and better data structure. For a given graph we create a data structure that suits best the goal of generating paths. Instead of having for each vertex a list of its neighbors we create data structure that will allow us to generate a candidate for next path in amortized $O(1)$ time.

We have an array $first$ in which for each vertex is written its first neighbor on its neighbors list, and an array $next$ of size $|V(G)|^2$ where for each pair of vertices $a, b$ if $a$ and $b$ are connected, there is written a neighbor of $a$ that is next after $b$ in a neighbors list of $a$ (or a flag indicating $b$ is $a$'s last neighbor). Then, say our input is a path $v = v_1, \ldots v_k$. If $next[v_{k-1}][v_k]$ exists we change $v_k$ to $next[v_{k-1}][v_k]$ and return $v$. If it indicated that $v_k$ is $v_{k-1}$'s last neighbor, we set $v_{k-1}$ to $next[v_{k-2}][v_{k-1}]$ and then $v_k$ to $first[v_{k-1}]$ (or we go further back, if all neighbors of $v_{k-2}$ are done).

This simple change in data structure design gave us a speedup of overall running time of our algorithm in the range of 20x\todo{check this}, with much bigger speedup for na\"ive algorithm (around TODOx).

\paragraph{Array unique}

For each path candidate we call a subroutine to determine whether all vertices in it are unique. As this subroutine could be called many times for generating a single path, its optimization is very important.

A theoretically optimal solution in general would be to have a hashing set of vertices. For each vertex in a path candidate, we check if it is already in a set. If it is, return that not all vertices are unique, else add it to the set.

In our use case, a few optimizations can be made. First, we don't need a hashing set. We can have an array of bools, of size $|V(G)|$ and mark vertices of a path there. Paths are usually much shorter than $|V(G)|$, but we operate on small graphs, so we can afford this. Second, we notice that we don't need to create this array for each call of $unique$ procedure. Let's instead have a static array $stamp$ of ints and a static $counter$ of how many times we called $unique$ method. For each element of a path $v_i$, if the value of $stamp$ array is equal to $counter$ we report that vertices are not unique. Else we set $stamp[v_i] = counter$. When returning from the $unique$ method we increment $counter$.

This optimization alone is crucial for performance. In our testing, all calls of $unique$ took abour 70\% of total running time of the algorithm. After using static $stamp$ array, it fell to 5.2\%. The overall speedup of the algorithm was about 6x. \todo{check if for sure it didn't use path speedup} The speedup of na\"ive algorithm was even greater -- about 20x.

\paragraph{Other uses for path generation}
\label{sec:usesGeneration}

Because of a good performance of our optimized algorithm for generating paths use it whenever possible. It can be easily modified to generate all possibly chordal paths or holes and therefore has much use in the algorithm. For example, when searching for Jewels we generate with it all possibly chordal paths of length 5 and for each check if it has required properties of a Jewel. This proved to be much faster than generating vertices that don't have forbidden chords one by one. We use the same algorithm for generating starting vertices for a possible $T_2$ and $T_3$.


\subsection{Correctness Testing}

\subsubsection{Unit tests}

In an algorithm this complex, debugging can be difficult and time consuming, so we used extensive unit testing and principles of test driven development to make sure that the program results are correct.

Whole algorithm was divided into subroutines used in many places. One of such subroutines is a path generation algorithm described above (\cref{sec:usesGeneration}). There are many other generalized methods we implemented: checking if a set of vertices is $X$-complete, getting a set of components of a graph, creating a induced graph or finding a shortest path, such that each internal vertex satisfies a predicate. Each of those and many more methods have unit tests that check their general use and edge cases. This allowed us to debug very effectively and have complex algorithms be simple to analyze.

In addition to correctness checking, extensive unit test suite allowed us to optimize our algorithm and test different subroutines with ease, without fear of introducing bugs.

\subsubsection{End to end tests}

In addition to unit tests, we employ a range of end to end tests, that check the final answer of the algorithm. We compare answer to the na\"ive algorithm's answer and also to the result of the algorithm on CUDA (\cref{sec:CUDA}).

Also, we test the algorithm on graphs that we know are perfect -- such as bipartite graphs, line graphs of bipartite graphs and complements thereof. In addition to that, a test on all perfect graphs up to a size of 10\todo{update to 11?} was performed -- we used \cite{graphRepo} as a source of perfect graphs.

We also test on graphs that we know are not perfect: we generate them by generating an odd hole and adding it with some edges to a random graph.

In total over 100 \todo{update the munber}cpu-hours of end to end tests were performed without any errors.

\subsection{Parallelism with CUDA (?)}
\label{sec:CUDA}

\TODO{a section?}
\TODO{CUDA background}
\TODO{moderngpu - allows us to run simple transforms}
\TODO{Implementing get NC is tough -- we rely on set<bitset> to do the work. Maybe we could use a dynamic set from Adam's link? As a "open question"}

We posed a question if it would be profitable to use GPU for this problem. On one hand the graphs we are working on are small and our time complexity so large that a speedup from massively parallel architecture could be profitable. On the other the algorithm is complex and not easy to parallelise. Therefore we decided to implement whole na\"ive algorithm and most time consuming parts of the CCLSV\todo{a good name for it?} algorithm.

\subsubsection{Na\"ive parallelisation}

Na\"ive algorithm is very simple -- generate all possible odd hole candidates, check if any of them is an odd hole and repeat for the complement. But its parallelisation is not trivial.

The biggest problem of implementing na\"ive algorithm is in splitting the work between CUDA threads. This is very important because while GPU is faster when all its cores are working efficiently, single core performance is much slower than CPU. But it is not obvious how to split the work.

Our first attempt was to switch our slightly more sophisticated way of generating paths (\cref{sec:usesGeneration}) in favor of generating all combinations. We could code a prefix of a combination as a number (we use combinations with repetitions for simplicity). This would give us $|V(G)|^k$ codes for all prefixes of length $k$. Then each CUDA thread would process its part of all codes: for each code it would first check if the encoded prefix is a valid path and then generate all path candidates with that prefix. This approach has two problems: it doesn't split the work evenly enough and it does too much unneeded work (only a few prefix codes are valid). Still, implementing it on GPU gives us a program with speeds comparable to na\"ive CPU algorithm after all optimizations.

A better approach is to generate all paths of some length $k$ on CPU (using algorithm described in \cref{sec:usesGeneration}), copy them on GPU and have each thread process them as described above. This is superior to the previous algorithm in utilizing GPU -- each prefix takes similar time to process. We also even out the work done by each thread by having single thread process multiple prefixes. But it puts much more strain on the CPU -- it has to generate all valid prefixes. Here we used experiments to determine best value of $k$ (it was 7 for graphs of $n < 15$ \todo{check} and 6 for larger). This yields an algorithm that beats CPU na\"ive by a factor of 50x. \todo{check}.

\subsubsection{CCLSV parallelisation}

To identify parts of CCLSV to implement, we used callgrind and found potential bottlenecks. The initial tests showed that with growing size of a graph some parts take more and more relative time. Because of callgrind's slow execution times compared to just running the program, we used manual timers for bigger tests. Using this method we identified testing all possible near-cleaners (\cref{alg:testNearCleaner}) as the biggest bottleneck for larger graphs. We considered two approaches: run whole \cref{alg:testNearCleaner} on a single CUDA core, testing multiple $X$s in parallel, or parallelize \cref{alg:testNearCleaner} itself and run it on one $X$ at a time. It turned out that second approach is better and much simpler.

Let us recall the \cref{alg:testNearCleaner}. It calculates array $R$ of shortest paths, then for each 3-vertex path and an additional vertex it does $O(1)$ checks to see if they along with two paths from $R$ give us an odd hole. We will parallelise the work after calculating $R$. Let us notice, that for all $X$s all 3-vertex paths are the same. We calculate them beforehand \todo{do it also in CPU} \todo{acutally do this}. Then each thread receives a 3-vertex path and an additional vertex and performs the required checks. This is almost perfect scenario for GPU -- we do a simple SIMD work in parallel, without a lot scattered memory access.

It turns out that this optimization alone speeds up \cref{alg:testNearCleaner} almost 30x \todo{check} on bigger tests, which gives us a speedup of about 5x \todo{check} for overall algorithm.

\TODO{implement other CUDA opts and describe them}

\subsection{Experiments}

\begin{figure}
  \centering
  \input{plots/hypercubes16to50.t.in.out.csv.lines.pgf}
  \caption{testPlot 1}
  % \label{fig:LCA_batch}
\end{figure}

Naive algorithm - brief description, bottlenecks optimizations (makes huge difference).\\

Description of tests used.\\

Results and Corollary - almost usable algorithm.


\section{Coloring Berge Graphs}

\subsection{Ellipsoid method}

We used an open source CSDP \cite{csdpRepo, csdp1999} library, that implements predictor corrector variant of the semidefinite programming algorithm to calculate $\vartheta(G)$, given $G$ on the input. The CSDP library has been used in many recent publications across different fields, such as \cite{Ampountolas_2017, Adasme_2011}.

Calculating $\vartheta(G)$ is the most complicated part of the coloring algorithm. With that done by an external library, the rest of the program is a straightforward implementation of the algorithms in \cref{sec:coloringEllipsoid}. Our main goal of the implementation was to check if this method is still impractical, even on modern equipment. Let us proceed straight to experiments and results.

\subsubsection{Experiments and results}

\subsection{Combinatorial Method}

Cite the paper.\\

On its complexity - point to appendix for pseudo-code.