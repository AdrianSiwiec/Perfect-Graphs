A repository containing the discussed source code and tests is available under the link: \href{https://github.com/AdrianSiwiec/Perfect-Graphs}{\texttt{https://github.com/AdrianSiwiec/Perfect-Graphs}}
\todo{make repo public and add readme on howto run, some script creating and running default tests}

\section{Berge graphs recognition: na\"ive approach}
When implementing an algorithm with the running time of $O(|V|^9)$ (especially one as complex as CCLSV), one of the first questions that pop up is whether this algorithm is at all usable. We couldn't find any existing programs that test whether the graph is perfect, be it an implementation of CCLSV, or a na\"ive approach.\todo{well.. there's this java impl} So, to test the usability of CCLSV implementation, we measured it against our own na\"ive perfect recognition algorithm. Let us describe it briefly.

We try find an odd hole directly, then, we run the same algorithm on $\overline{G}$, and if no odd hole is found we report that the graph is perfect, else that it is not.

To find an odd hole, we use a backtracking technique. We will enumerate all paths and holes, and for each test if it is an odd hole. A partial solution to our backtracking problem is a path in $G$. To move forward from a partial solution, we append consecutively to its end all neighbors of current path's last vertex and continue recursively after each. When a current solution is an odd hole we return it. When we exhaust all possible paths from the current partial solution, we remove its last vertex and continue.

This backtracking algorithm is very simple, but has a great potential for optimizations in its implementation. We also use path enumeration algorithm in the CCLSV algorithm, so we give much attention to its optimization.

\subsubsection{Path enumerating optimizations}
Now let's turn our attention to enumerating all paths. This is a generalization of the problem of finding an odd hole, if we allow a path to have a single additional edge between the first and the last of its vertices if and only if the hole formed would be odd.

First of all, there are many methods for generating all possible paths. We could simply enumerate all sequences of vertices without repetition and for each one of them check if it is a path, i.e., if all pairs of vertices next to each other are connected and that there are no other edges, but this would be too slow and render our algorithm unusable.

We will create a method that enumerates all paths in some order. We notice that returning a new path for each call of the enumeration method is wasteful. Therefore, we need a method that receives a reference to a path on the input (or a special flag indicating it should generate first path) and returns next path in some order (or a first path, or a code signaling all paths have been generated) using memory from the input. As this method will be used many, many times we require of it to work in place with constant additional space.

With those requirements defined a simplest algorithm would be to implement a sort of a "counter" with base of $|V(G)|$. In short: we take a path on a input, increment its last vertex until it is a neighbor of the vertex one before last. Then we check if generated sequence is a path. If it is we return it and if we run out of vertices before a path is found, we increment one before last vertex until it is a neighbor of one vertex before it, set last vertex to first and continue the process. If still no path is found, we increment vertices closer to beginning of the path, until all a path is found or we check all candidates.

But we can do much better with some care and a suitable data structure. In addition to having for each vertex a list of its neighbors we create data structure that will allow us to generate a candidate for next path in amortized $O(1)$ time.

We have an array $first$ in which for each vertex is written its first neighbor on its neighbors list, and an array $next$ of size $|V(G)|^2$ where for each pair of vertices $a, b$ if $a$ and $b$ are connected, there is written a neighbor of $a$ that is next after $b$ in a neighbors list of $a$ (or a flag indicating $b$ is $a$'s last neighbor). Then, say our input is a path $v = v_1, \ldots v_k$. If $next[v_{k-1}][v_k]$ exists we change $v_k$ to $next[v_{k-1}][v_k]$ and return $v$. If it indicated that $v_k$ is $v_{k-1}$'s last neighbor, we set $v_{k-1}$ to $next[v_{k-2}][v_{k-1}]$ and then $v_k$ to $first[v_{k-1}]$ (or we go further back, if all neighbors of $v_{k-2}$ are done).

This simple change in data structure design gave us a speedup of overall running time of our na\"ive algorithm in the range of about 5x.

Another crucial optimization is to eliminate partial solutions that have edges between not consecutive vertices right away. Also, when we look for an odd hole specifically, we can assume that its first vertex we enumerate is the lowest. Those two optimizations make a huge difference. With them, our na\"ive algorithm implementation went from being able to check graphs with $|V|$ up to 12-14, within a reasonable time, to being faster than CCLSV on most of our tests.

\paragraph{Array unique.}

For each path candidate $v$ we call a subroutine \texttt{areUnique}$(v)$ to determine whether all vertices in it are unique. As this subroutine could be called many times for generating a single path, its optimization is very important.

A theoretically optimal solution in general would be to have a hashing set of vertices. For each vertex in a path candidate, we check if it is already in a set. If it is, return that not all vertices are unique, else add it to the set.

In our use case, a few optimizations can be made. First, we don't need a hashing set. We can have an array of bools, of size $|V(G)|$ and mark vertices of a path there. Paths are usually much shorter than $|V(G)|$, but we operate on small graphs, so we can afford this. Second, we notice that we don't need to create this array for each call of \texttt{areUnique} procedure. Let's instead have a static array $stamp$ of ints and a static $counter$ of how many times we called \texttt{areUnique} method. For each element of a path $v_i$, if the value of $stamp$ array is equal to $counter$ we report that vertices are not unique. Else we set $stamp[v_i] = counter$. When returning from \texttt{areUnique} method we increment $counter$.

This optimization alone almost cuts down na\"ive algorithm's running time in half.

\paragraph{Other uses for path generation.}
\label{sec:usesGeneration}

Because of a good performance of our optimized algorithm for generating paths, we use it whenever possible. It can be easily modified to enumerate all ''paths'' that can have additional edges or to enumerate holes and therefore has much use in the CCLSV algorithm. For example, when searching for jewels we generate with it all ''paths'' with possible additional edges, of length 5 and for each check if it has required properties of a jewel. This proved to be much faster than generating vertices and checking their properties one by one. We use the same algorithm for generating starting vertices for a possible $\T_2$ and $\T_3$. Before optimizing path enumeration algorithm, all calls of $areUnique$ alone took abour 70\% of total running time of the CCLSV algorithm. After optimizing $areUnique$, path enumeration still took more than 50\% of total running time. After all optimizations it is almost unnoticable.

With all those optimizations made, our na\"ive algorithm proved to be quite fast in some cases, although its running time is very dependent on the properties of the input, as we should expect from an optimized non-polynomial algorithm. See \Cref{sec:experiments} for the running times.

\section{Berge graphs recognition: polynomial algorithm}

The Berge recognition algorithm's running time is $O(|V|^9)$, which brings into question its applicability to any real use case. Although time complexity is indeed a limiting factor, a number of lower level optimizations done on implementation's level make a very big difference and make it more viable than na\"ive algorithm, at least on some test cases. We also explore a new frontier of implementing its most time consuming part on massively parallel GPU architecture, with some good results (\Cref{sec:CUDA}).

\subsection{Optimizations}
\label{sec:Optimizations}

When implementing a complicated algorithm that has a time complexity of $O(|V|^9)$ optimizations can be both crucial and difficult to implement. There is not a single code path that takes up all the running time -- or at least there isn't one from a theoretical point of view. Therefore a tool for inspecting running time bottlenecks is needed. We used Valgrind's tool called \emph{callgrind} \cite{callgrind}.

% \TODO{Caly ponizszy akapit wymaga przeformulowania/skrocenia, nie trzeba wyjasniac detali dzialania callgrinda - tylko napisac ze zlicza event county wg funkcji + ze mozna to wizualizowac}

Callgrind is a profiling tool that records the call history and event counts (data reads, cache misses etc.) of a program and presents it as a call-graph. It is then possible to visualise this, we used a tool \emph{gprof2dot} \cite{gprof2dot} to generate visual call graphs from callgrind's output.

Before any optimizatios, a major part of Berge recognition algorithm was being spent on enumerating all possible paths of given length -- this is done either to find a simple structure or check a possible near cleaner for an amenable odd hole. Therefore optimizations of the path enumerating algorithm were also helpful in speeding up CCLSV implementation.

% \emergencystretch=10em

Another algorithm for which we found major speedups is the algorithm to generate all possible near-cleaners (\Cref{alg:listNearCleaners}). We optimized it by using a a \texttt{dynamic\_bitset} from the boost library \cite{boost}. It is a data structure to represent a set of bits, that also allows for fast bitwise operators that one can apply to builtin integers. At the very end of \Cref{alg:listNearCleaners} a set $\mathcal{R}$ is constructed, which is an union of all possible unions of pairs $X_i$ and $N_j$. We used \texttt{dynamic\_bitset} to represent each of the sets $X_i$ and $N_j$, and the elements of the set $\mathcal{R}$. This significantly speeds up the operation of union of two sets (line \ref{line:listNCcalcR}). With this modification, the speedup of \Cref{alg:listNearCleaners} was about 20\%.

After these optimizations, checking each potential near-cleaner by \Cref{alg:testNearCleaner} is by far the biggest bottleneck of the CCLSV algorithm. We didn't find any major optimizations there, but the \Cref{alg:testNearCleaner} has a good potential for parrallelization, which we explore in \Cref{sec:CUDA}.

\subsection{Correctness testing}

\subsubsection{Unit tests}

In an algorithm this complex, debugging can be difficult and time consuming, so we used extensive unit testing and principles of test driven development to make sure that the program results are correct.

Whole algorithm was divided into subroutines used in many places. One of such subroutines is a path generation algorithm described above (\Cref{sec:usesGeneration}). There are many other generalized methods we implemented: checking if a set of vertices is $X$-complete, getting a set of components of a graph, creating a induced graph or finding a shortest path, such that each internal vertex satisfies a predicate. Each of those and many more methods have unit tests that check their general use and edge cases. This allowed us to debug very effectively and have complex algorithms be simple to analyze.

In addition to correctness checking, extensive unit test suite allowed us to optimize our algorithm and test different subroutines with ease, without the fear of introducing bugs.

\subsubsection{End to end tests}

In addition to unit tests, we employ a range of end to end tests, that check the final answer of the algorithm. We compare answer to the na\"ive algorithm's answer and also to the result of the algorithm on CUDA (\Cref{sec:CUDA}).

Also, we test our implementations on graphs that we know are perfect -- such as bipartite graphs, line graphs of bipartite graphs and complements thereof. In addition to that, a test on all perfect graphs up to a size of 10 was performed -- we used \cite{graphRepo} as a source of perfect graphs.

We also test on graphs that we know are not perfect: we generate them by generating an odd hole and joining it with some edges to a random graph.

In total over 200 CPU-hours of end to end tests were performed without any errors.

\section{Parallelism with CUDA}
\label{sec:CUDA}

CUDA is a parrallel computing platform model created by Nvidia and executed on Nvidia GPUs. It allows to run a program  by many CUDA threads at once. Although usually described by the the PRAM models, implementing PRAM algorithm in CUDA architecture is not as straightforward as implementing a RAM algorith on a CPU. Some of the causes are: synchronization between threads is very costly, there are multiple memory models to choose from, threads run divided in blocks and it is possible to efficiently share memory within blocks and copying memory from and back to CPU suffers from a large latency. What is more, there are no standard dynamic structures in the PRAM model, let alone CUDA, that are taken for granted in the RAM model, such as hashing maps or sets. 
% We used a library called \emph{moderngpu} for our implementation, because it allows a slightly cleaner code.

% \TODO{Implementing get NC is tough -- we rely on set<bitset> to do the work. Maybe we could use a dynamic set from Adam's link? As a "open question"}

We posed a question if it would be profitable to use GPU for the problem of recognizing Berge graphs. On one hand the graphs we are working on are small and our time complexity so large that a speedup from massively parallel architecture could be profitable. On the other hand, the algorithm is complex and not easy to parallelise. Therefore we decided to analyze the  CCLSV algorithm and parrallelize parts that would benefit from it the most.

% \subsubsection{Na\"ive parallelisation}

% Na\"ive algorithm is very simple -- generate all possible odd hole candidates, check if any of them is an odd hole and repeat for the complement. But its parallelisation is not trivial.

% The biggest problem of implementing na\"ive algorithm is in splitting the work between CUDA threads. This is very important because while GPU is faster when all its cores are working efficiently, single core performance is much slower than CPU. But it is not obvious how to split the work.

% Our first attempt was to switch our slightly more sophisticated way of generating paths (\cref{sec:usesGeneration}) in favor of generating all combinations. We could code a prefix of a combination as a number (we use combinations with repetitions for simplicity). This would give us $|V(G)|^k$ codes for all prefixes of length $k$. Then each CUDA thread would process its part of all codes: for each code it would first check if the encoded prefix is a valid path and then generate all path candidates with that prefix. This approach has two problems: it doesn't split the work evenly enough and it does too much unneeded work (only a few prefix codes are valid). Still, implementing it on GPU gives us a program with speeds comparable to na\"ive CPU algorithm after all optimizations.

% A better approach is to generate all paths of some length $k$ on CPU (using algorithm described in \cref{sec:usesGeneration}), copy them on GPU and have each thread process them as described above. This is superior to the previous algorithm in utilizing GPU -- each prefix takes similar time to process. We also even out the work done by each thread by having single thread process multiple prefixes. But it puts much more strain on the CPU -- it has to generate all valid prefixes. Here we used experiments to determine best value of $k$ (it was 7 for graphs of $n < 15$ \todo{check} and 6 for larger). This yields an algorithm that beats CPU na\"ive by a factor of 50x. \todo{check}.

% \subsubsection{CCLSV parallelisation}

% \TODO{nizgrabnie, przepisaÄ‡ ponizszy}

We identified testing all possible near-cleaners (\Cref{alg:testNearCleaner}) as the biggest bottleneck for larger graphs. We considered two approaches: run whole \Cref{alg:testNearCleaner} on a single CUDA core, testing multiple $X$s in parallel, or parallelize \Cref{alg:testNearCleaner} itself and run it on one $X$ at a time. It turned out that second approach is better and much simpler.

Let us recall the \Cref{alg:testNearCleaner}. It calculates array $R$ of shortest paths, then for each 3-vertex path and an additional vertex it does $O(1)$ checks to see if they along with two paths from $R$ give us an odd hole. We will parallelise the work after calculating $R$, that is, lines \ref{line:cudaStart}-\ref{line:cudaEnd}. Let us notice, that for all $X$s, all 3-vertex paths enumerated in line \ref{line:3vertex} are the same. We calculate them beforehand, then each CUDA thread receives one 3-vertex path $x_1 - x_2 - x_3$ and an additional vertex $y_1$ and performs the required checks in lines \ref{line:cudaWorkStard}-\ref{line:cudaWorkEnd}. This is almost perfect scenario for the GPU -- we do a simple SIMD work in parallel, without a lot scattered memory access.

The part of the CCLSV implemented on the GPU achieved speedup of up to 15x. Which gave us a speedup of up to 9x for overall CCLSV algorithm, depending on a class of graph we benchmark on.

